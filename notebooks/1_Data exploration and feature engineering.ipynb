{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of DataMesh: Data exploration and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML. AWS Lake Formation provides a single place to define, classify, tag, and manage fine-grained permissions for data in Amazon S3.\n",
    "\n",
    "In this SageMaker Studio notebook we highlight how you can do data exploration and feature engineering in a data mesh environment. The producer account has provided access to the consumer account, and the database and table are available in the consumer AWS Lake Formation (LF). The next step before going through this notebook, is to grant access to the table in LF to the SageMaker role used by your notebook. LF is an extra layer of access management seating on the top of IAM. This means that even if you have access to the table or database with IAM, if you did not grant access with LF, you will not be able to access the data.\n",
    "\n",
    "In a Data mesh environment you use Amazon Athena to access the data. Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. In this notebook we use Athena to first explore a sample of the data within the notebook. After, we use Athena within a processing job to query the whole data and perform the tranformations. \n",
    "\n",
    "This sample notebook walks you through: \n",
    "1. Query and explore credit risk dataset shared by the producer account - [South German Credit (UPDATE) Data Set](https://archive.ics.uci.edu/ml/datasets/South+German+Credit+%28UPDATE%29)\n",
    "2. Preprocessing data with sklearn on the dataset and build a model to featurize future data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize and import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('athena')\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker import Session\n",
    "from sagemaker.xgboost import XGBoostModel\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "region = boto3.session.Session().region_name\n",
    "role = get_execution_role()\n",
    "import sys\n",
    "!{sys.executable} -m pip install PyAthena\n",
    "import pyathena\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "from io import StringIO\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import IPython\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "session = Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"sagemaker/sagemaker-credit-risk-model-data-mesh\"\n",
    "region = session.boto_region_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install awswrangler\n",
    "import awswrangler as wr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data exploration\n",
    "In this part we will do some data exploration on the notebook itself. We first query Athena by using the pyathena library and then we save the results as a dataframe to later explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Athena query from boto3\n",
    "#Once you have queried the data, use the CLI command in the terminal to get the results \n",
    "#aws athena get-query-results --query-execution-id ADD HERE THE EXECUTION ID TO GET THE RESULTS\n",
    "response = client.start_query_execution(\n",
    "    QueryString='SELECT * FROM \"rl_credit-card\".\"credit_card\" LIMIT 10',\n",
    "    QueryExecutionContext={\n",
    "        'Database': 'rl_credit-card',\n",
    "        'Catalog': 'AwsDataCatalog'\n",
    "    },\n",
    "    ResultConfiguration={\n",
    "        'OutputLocation': 's3://sagemaker-us-east-1-934586227363/athenaqueries'\n",
    "    }\n",
    ")\n",
    "print(response['QueryExecutionId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aws athena get-query-results --query-execution-id ADD HERE THE EXECUTION ID PRINTED ABOVE TO GET THE RESULTS\n",
    "#!aws athena get-query-results --query-execution-id '79a497d7-8240-4d98-b33a-5fefa47a8fb4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can as well query via pyathena to avoid the two steps way from above\n",
    "\n",
    "conn = connect(s3_staging_dir='s3://sagemaker-us-east-1-934586227363/athenaqueries/',\n",
    "               region_name='us-east-1')\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM \"rl_credit-card\".\"credit_card\" LIMIT 10;', conn)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= wr.athena.read_sql_query('SELECT * FROM credit_card LIMIT 10;', database=\"rl_credit-card\", ctas_approach=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there is class unbalance\n",
    "\n",
    "df['credit_risk'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    role=role,\n",
    "    base_job_name=\"sagemaker-clarify-credit-risk-processing-job\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    framework_version=\"0.20.0\",\n",
    ")\n",
    "!pygmentize processing/preprocessor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition\n",
    "from sagemaker.dataset_definition.inputs import DatasetDefinition\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "raw_data_path = \"s3://{0}/{1}/data/train/\".format(bucket, prefix)\n",
    "train_data_path = \"s3://{0}/{1}/data/preprocessed/train/\".format(bucket, prefix)\n",
    "val_data_path = \"s3://{0}/{1}/data/preprocessed/val/\".format(bucket, prefix)\n",
    "model_path = \"s3://{0}/{1}/sklearn/\".format(bucket, prefix)\n",
    "test_data_path = \"s3://{0}/{1}/data/test/\".format(bucket, prefix)\n",
    "\n",
    "AthenaDataset = AthenaDatasetDefinition (\n",
    "  catalog = 'AwsDataCatalog', \n",
    "  database = 'rl_credit-card', \n",
    "  query_string = 'SELECT * FROM \"rl_credit-card\".\"credit_card\"',                                \n",
    "  output_s3_uri = 's3://sagemaker-us-east-1-934586227363/athenaqueries/', \n",
    "  work_group = 'primary', \n",
    "  output_format = 'PARQUET')\n",
    "\n",
    "dataSet = DatasetDefinition(\n",
    "  athena_dataset_definition = AthenaDataset, \n",
    "  local_path='/opt/ml/processing/input/dataset.parquet')\n",
    "\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"processing/preprocessor.py\",\n",
    "    inputs=[ProcessingInput(\n",
    "      input_name=\"dataset\", \n",
    "      destination=\"/opt/ml/processing/input\", \n",
    "      dataset_definition=dataSet)],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train_data\", source=\"/opt/ml/processing/train\", destination=train_data_path\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"val_data\", source=\"/opt/ml/processing/val\", destination=val_data_path\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"model\", source=\"/opt/ml/processing/model\", destination=model_path\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test_data\", source=\"/opt/ml/processing/test\", destination=test_data_path\n",
    "        ),\n",
    "    ],\n",
    "    arguments=[\"--train-test-split-ratio\", \"0.2\"],\n",
    "    logs=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize training/train_xgboost.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.1\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"silent\": \"1\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"early_stopping_rounds\": \"20\",\n",
    "}\n",
    "\n",
    "entry_point = \"train_xgboost.py\"\n",
    "source_dir = \"training/\"\n",
    "output_path = \"s3://{0}/{1}/{2}\".format(bucket, prefix, \"xgb_model\")\n",
    "code_location = \"s3://{0}/{1}/code\".format(bucket, prefix)\n",
    "\n",
    "estimator = XGBoost(\n",
    "    entry_point=entry_point,\n",
    "    source_dir=source_dir,\n",
    "    output_path=output_path,\n",
    "    code_location=code_location,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    instance_count=1,\n",
    "    framework_version=\"0.90-2\",\n",
    "    py_version=\"py3\",\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"credit-risk-xgb-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    \"s3://{0}/{1}/data/preprocessed/train/\".format(bucket, prefix), content_type=\"csv\"\n",
    ")\n",
    "val_input = TrainingInput(\n",
    "    \"s3://{0}/{1}/data/preprocessed/val/\".format(bucket, prefix), content_type=\"csv\"\n",
    ")\n",
    "\n",
    "inputs = {\"train\": train_input, \"validation\": val_input}\n",
    "\n",
    "estimator.fit(inputs, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
